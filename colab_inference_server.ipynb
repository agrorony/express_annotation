{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "g9etB6cc_xZU",
      "metadata": {
        "id": "g9etB6cc_xZU"
      },
      "source": [
        "# SAM Inference Server (FastAPI + ngrok)\n",
        "\n",
        "This notebook mounts Google Drive, loads a SAM checkpoint, verifies CUDA, and starts a FastAPI inference server exposed via ngrok.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "YdX59oRm_xZW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdX59oRm_xZW",
        "outputId": "18429472-558a-4c18-f31e-7e016f590de2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 1) Mount Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cSd0xzoT_xZY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSd0xzoT_xZY",
        "outputId": "fa8ca988-dce9-4fef-a6d0-504ac66168ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint path: /content/drive/MyDrive/soil_microCT_images/drive_scripts/napari_loader/checkpoints/sam_vit_h_4b8939.pth\n"
          ]
        }
      ],
      "source": [
        "# 2) Configure checkpoint path (set DRIVE_SUBPATH env var if needed)\n",
        "import os\n",
        "\n",
        "DRIVE_SUBPATH = 'soil_microCT_images/drive_scripts/napari_loader/checkpoints'\n",
        "checkpoint_path = f\"/content/drive/MyDrive/{DRIVE_SUBPATH}/sam_vit_h_4b8939.pth\"\n",
        "print(\"Checkpoint path:\", checkpoint_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "xLvayyWA_xZY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLvayyWA_xZY",
        "outputId": "45bef13a-f3d7-4529-b72c-8ab933e9e2d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# 3) Install dependencies\n",
        "!pip -q install fastapi uvicorn pyngrok git+https://github.com/facebookresearch/segment-anything.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "zboapnwk_xZZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zboapnwk_xZZ",
        "outputId": "cfcecf98-f9fc-4e2c-8434-587cca2d3acd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA is available.\n"
          ]
        }
      ],
      "source": [
        "# 4) Verify CUDA availability\n",
        "import torch\n",
        "\n",
        "assert torch.cuda.is_available() is True, \"CUDA is not available. Please switch to a GPU runtime.\"\n",
        "print(\"CUDA is available.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "osqcmrf5_xZZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osqcmrf5_xZZ",
        "outputId": "394bde38-d95f-441a-d4ef-843491eaff6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SAM model loaded.\n"
          ]
        }
      ],
      "source": [
        "# 5) Load SAM model\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "if not os.path.isfile(checkpoint_path):\n",
        "    raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "\n",
        "sam = sam_model_registry[\"vit_h\"](checkpoint=checkpoint_path)\n",
        "sam.to(device=\"cuda\")\n",
        "predictor = SamPredictor(sam)\n",
        "print(\"SAM model loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JRMETcm-_xZZ",
      "metadata": {
        "id": "JRMETcm-_xZZ"
      },
      "outputs": [],
      "source": [
        "# 6) Define FastAPI app\n",
        "import base64\n",
        "import io\n",
        "import threading\n",
        "from typing import Optional\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "app = FastAPI()\n",
        "_predictor_lock = threading.Lock()\n",
        "\n",
        "class PredictRequest(BaseModel):\n",
        "    image: str  # base64-encoded PNG\n",
        "    box: Optional[list] = None   # [x0, y0, x1, y1]\n",
        "    input_mask: Optional[str] = None  # base64-encoded PNG mask (optional)\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    import torch\n",
        "    return {\n",
        "        \"torch_available\": True,\n",
        "        \"torch_version\": torch.__version__,\n",
        "        \"cuda_available\": torch.cuda.is_available(),\n",
        "        \"cuda_device_count\": torch.cuda.device_count(),\n",
        "    }\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict(req: PredictRequest):\n",
        "    image_bytes = base64.b64decode(req.image)\n",
        "    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "    image_np = np.array(image)\n",
        "\n",
        "    # If an input_mask is provided, decode and binarize it (all non-zero -> foreground)\n",
        "    if req.input_mask is not None:\n",
        "        mask_bytes = base64.b64decode(req.input_mask)\n",
        "        mask_img = Image.open(io.BytesIO(mask_bytes))\n",
        "        mask_np = np.array(mask_img)\n",
        "        # If mask has channels, use the first channel\n",
        "        if mask_np.ndim > 2:\n",
        "            mask_np = mask_np[..., 0]\n",
        "        binary_mask = (mask_np > 0).astype(np.uint8)\n",
        "        # SAM predictor expects mask input as 0/1 or boolean\n",
        "        mask_for_predictor = binary_mask.astype(bool)\n",
        "        with _predictor_lock:\n",
        "            predictor.set_image(image_np)\n",
        "            masks, _, _ = predictor.predict(mask_input=mask_for_predictor, multimask_output=False)\n",
        "        mask = (masks[0] * 255).astype(np.uint8)\n",
        "    else:\n",
        "        # Fall back to box if provided\n",
        "        if req.box is None:\n",
        "            return {\"error\": \"Either 'box' or 'input_mask' must be provided\"}, 400\n",
        "        box = np.array(req.box, dtype=np.float32)\n",
        "        with _predictor_lock:\n",
        "            predictor.set_image(image_np)\n",
        "            masks, _, _ = predictor.predict(box=box, multimask_output=False)\n",
        "        mask = (masks[0] * 255).astype(np.uint8)\n",
        "\n",
        "    mask_img = Image.fromarray(mask)\n",
        "    buffer = io.BytesIO()\n",
        "    mask_img.save(buffer, format=\"PNG\")\n",
        "    mask_b64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "    return {\n",
        "        \"mask\": mask_b64,\n",
        "        \"mask_metadata\": {\n",
        "            \"shape\": list(mask.shape),\n",
        "            \"dtype\": str(mask.dtype),\n",
        "        },\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "WDh1juVT_xZa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDh1juVT_xZa",
        "outputId": "e555b33b-f715-4f07-cbb8-faf4798eb48d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Server started on port 8000.\n"
          ]
        }
      ],
      "source": [
        "# 7) Start FastAPI server in the background\n",
        "import threading\n",
        "import uvicorn\n",
        "\n",
        "def run_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "thread = threading.Thread(target=run_server, daemon=True)\n",
        "thread.start()\n",
        "print(\"Server started on port 8000.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "qzIyLpy0_xZa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzIyLpy0_xZa",
        "outputId": "7aeb1cfe-1257-4af4-8a04-89267f18e69d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"https://monogenetic-nonmalarial-nia.ngrok-free.dev\" -> \"http://localhost:8000\"\n"
          ]
        }
      ],
      "source": [
        "# 8) Expose server via ngrok\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "\n",
        "ngrok_token = userdata.get('NGROK')\n",
        "if not ngrok_token:\n",
        "    raise ValueError('NGROK_AUTHTOKEN environment variable is required.')\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "public_url = ngrok.connect(8000, \"http\")\n",
        "print(\"Public URL:\", public_url)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
