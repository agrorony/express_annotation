{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# SAM Inference Server (FastAPI + ngrok)\n\nThis notebook mounts Google Drive, loads a SAM checkpoint, verifies CUDA, and starts a FastAPI inference server exposed via ngrok.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 1) Mount Google Drive\nfrom google.colab import drive\n\ndrive.mount('/content/drive')\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 2) Configure checkpoint path (set DRIVE_SUBPATH env var if needed)\nimport os\n\nDRIVE_SUBPATH = os.environ.get('DRIVE_SUBPATH', 'PATH_TO_CHECKPOINT_FOLDER')\ncheckpoint_path = f\"/content/drive/MyDrive/{DRIVE_SUBPATH}/sam_vit_h_4b8939.pth\"\nprint(\"Checkpoint path:\", checkpoint_path)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 3) Install dependencies\n!pip -q install fastapi uvicorn pyngrok git+https://github.com/facebookresearch/segment-anything.git\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 4) Verify CUDA availability\nimport torch\n\nassert torch.cuda.is_available() is True, \"CUDA is not available. Please switch to a GPU runtime.\"\nprint(\"CUDA is available.\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 5) Load SAM model\nimport os\nimport numpy as np\nfrom PIL import Image\nfrom segment_anything import sam_model_registry, SamPredictor\n\nif not os.path.isfile(checkpoint_path):\n    raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n\nsam = sam_model_registry[\"vit_h\"](checkpoint=checkpoint_path)\nsam.to(device=\"cuda\")\npredictor = SamPredictor(sam)\nprint(\"SAM model loaded.\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 6) Define FastAPI app\nimport base64\nimport io\nimport threading\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n_predictor_lock = threading.Lock()\n\nclass PredictRequest(BaseModel):\n    image: str  # base64-encoded PNG\n    box: list   # [x0, y0, x1, y1]\n\n@app.get(\"/health\")\ndef health():\n    import torch\n    return {\n        \"torch_available\": True,\n        \"torch_version\": torch.__version__,\n        \"cuda_available\": torch.cuda.is_available(),\n        \"cuda_device_count\": torch.cuda.device_count(),\n    }\n\n@app.post(\"/predict\")\ndef predict(req: PredictRequest):\n    image_bytes = base64.b64decode(req.image)\n    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n    image_np = np.array(image)\n\n    box = np.array(req.box, dtype=np.float32)\n    with _predictor_lock:\n        predictor.set_image(image_np)\n        masks, _, _ = predictor.predict(box=box, multimask_output=False)\n\n    mask = (masks[0] * 255).astype(np.uint8)\n    mask_img = Image.fromarray(mask)\n    buffer = io.BytesIO()\n    mask_img.save(buffer, format=\"PNG\")\n    mask_b64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n    return {\n        \"mask\": mask_b64,\n        \"mask_metadata\": {\n            \"shape\": list(mask.shape),\n            \"dtype\": str(mask.dtype),\n        },\n    }\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 7) Start FastAPI server in the background\nimport threading\nimport uvicorn\n\ndef run_server():\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n\nthread = threading.Thread(target=run_server, daemon=True)\nthread.start()\nprint(\"Server started on port 8000.\")\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 8) Expose server via ngrok\nimport os\nfrom pyngrok import ngrok\n\nngrok_token = os.environ.get('NGROK_AUTHTOKEN', '').strip()\nif not ngrok_token:\n    raise ValueError('NGROK_AUTHTOKEN environment variable is required.')\nngrok.set_auth_token(ngrok_token)\n\npublic_url = ngrok.connect(8000, \"http\")\nprint(\"Public URL:\", public_url)\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}